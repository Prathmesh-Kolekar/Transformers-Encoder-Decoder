{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Multi_head_crossattn import MultiHeadCrossAttention\n",
    "from Multi_head_selfattn import MultiHeadSelfAttention\n",
    "from feedforward import FeedForward\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "  def __init__(self,n_heads,batch_size,sequence_len, n_embedding):\n",
    "    super().__init__()\n",
    "    self.n_heads = n_heads\n",
    "    self.batch_size = batch_size\n",
    "    self.sequence_len = sequence_len\n",
    "    self.n_embedding = n_embedding\n",
    "\n",
    "    self.qkv_layer = nn.Linear(n_embedding,3*self.n_embedding)\n",
    "    self.dropout1 = nn.Dropout(0.1)\n",
    "    self.attention = MultiHeadSelfAttention()\n",
    "    self.linear_layer = nn.Sequential(\n",
    "            nn.Linear(n_embedding,2*n_embedding),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2*n_embedding,n_embedding)\n",
    "        )\n",
    "    self.layer_norm1 = nn.LayerNorm(n_embedding)\n",
    "\n",
    "    self.q_layer = nn.Linear(n_embedding,self.n_embedding)\n",
    "    self.kv_layer = nn.Linear(n_embedding,2*self.n_embedding)\n",
    "    self.cross_attention = MultiHeadCrossAttention(n_embedding)\n",
    "    self.dropout2 = nn.Dropout(0.1)\n",
    "    self.layer_norm2 = nn.LayerNorm(n_embedding)\n",
    "\n",
    "    self.fcl = FeedForward(n_embedding)\n",
    "    self.dropout3 = nn.Dropout(0.1)\n",
    "    self.layer_norm3 = nn.LayerNorm(n_embedding)\n",
    "\n",
    "  def forward(self,x,y,mask=None):\n",
    "    residual = y\n",
    "    y = self.qkv_layer(y)\n",
    "    print(y.shape)\n",
    "    y = y.view(self.batch_size,self.sequence_len,self.n_heads,3*self.n_embedding//self.n_heads)\n",
    "    y = y.permute(0,2,1,3)\n",
    "    y = self.attention(y)\n",
    "    y= y.reshape(self.batch_size,self.sequence_len,self.n_embedding)\n",
    "    y = self.dropout1(y)\n",
    "    # y = y + residual\n",
    "    y = self.layer_norm1(y+residual)\n",
    "\n",
    "    residual = y\n",
    "    q = self.q_layer(y)\n",
    "    kv = self.kv_layer(x)\n",
    "    q=q.view(self.batch_size,self.sequence_len,self.n_heads,self.n_embedding//self.n_heads)\n",
    "    kv = kv.view(self.batch_size,self.sequence_len,self.n_heads,2*self.n_embedding//self.n_heads)\n",
    "    q = q.permute(0,2,1,3)\n",
    "    kv = kv.permute(0,2,1,3)\n",
    "    y = self.cross_attention(q,kv,mask)\n",
    "    y = y.reshape(self.batch_size,self.sequence_len,self.n_embedding)\n",
    "    # y = nn.Linear(2*n_embedding,n_embedding)\n",
    "    y = self.linear_layer(y)\n",
    "    y = self.dropout2(y)\n",
    "    # y = y + residual\n",
    "    y = self.layer_norm2(y+residual)\n",
    "\n",
    "    residual = y\n",
    "    y = self.fcl(y)\n",
    "    y = self.dropout3(y)\n",
    "    # y = y + residual\n",
    "    y = self.layer_norm3(y+residual)\n",
    "    return y\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "  def __init__(self,n_layers,n_heads,batch_size,sequence_len, n_embedding):\n",
    "    super().__init__()\n",
    "    self.layers = nn.ModuleList([TransformerDecoderLayer(n_heads,batch_size,sequence_len, n_embedding) for _ in range(n_layers)])\n",
    "\n",
    "  def forward(self,x,y):\n",
    "    for idx , layer in enumerate(self.layers):\n",
    "      print(f'Layer : {idx+1}')\n",
    "      y = layer(x,y)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_heads = 8\n",
    "batch_size = 4\n",
    "sequence_len = 4\n",
    "n_embedding = 512\n",
    "n_layers = 5\n",
    "\n",
    "qkv = torch.randn(batch_size,sequence_len,n_embedding*3)\n",
    "\n",
    "qkv = qkv.reshape(batch_size,sequence_len,n_heads,3*n_embedding//n_heads)\n",
    "\n",
    "qkv = qkv.permute(0,2,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'TransformerDecoderLayer' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22804/3066006843.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTransformerDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_heads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msequence_len\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msequence_len\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msequence_len\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1510\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1511\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1518\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1522\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22804/3121926545.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     68\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m       \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Layer : {layer+1}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m       \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'TransformerDecoderLayer' and 'int'"
     ]
    }
   ],
   "source": [
    "decoder = TransformerDecoder(n_layers,n_heads,batch_size,sequence_len,n_embedding)\n",
    "output = decoder(torch.randn(batch_size,sequence_len,n_embedding),torch.randn(batch_size,sequence_len,n_embedding))\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
